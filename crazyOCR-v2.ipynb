{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6fdaa-b942-4762-bed0-6ba3deb7a259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports and Environment Setup ---\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94f431-05bd-40a9-b51c-f0dfbf3a66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tweakable Parameters ---\n",
    "MIN_AREA_FRAC = 0.08\n",
    "CANNY_LOW = 50\n",
    "CANNY_HIGH = 150\n",
    "GAUSSIAN_BLUR_KERNEL = (3, 3)\n",
    "DILATE_KERNEL_SIZE = (3, 3)\n",
    "DILATE_ITERATIONS = 1\n",
    "\n",
    "# --- OCR preprocessing ---\n",
    "BILATERAL_D = 5\n",
    "BILATERAL_SIGMA_COLOR = 50\n",
    "BILATERAL_SIGMA_SPACE = 30\n",
    "CLAHE_CLIP_LIMIT = 3.0\n",
    "CLAHE_TILE_GRID = (8, 8)\n",
    "TARGET_SHORT_SIDE = 1320\n",
    "\n",
    "DO_INVERT = False\n",
    "DO_BINARIZE = False\n",
    "ERODE_KERNEL_SIZE = (2, 2)\n",
    "ERODE_ITERATIONS = 3\n",
    "DILATE_KERNEL_SIZE_TEXT = (3, 3)\n",
    "DILATE_ITERATIONS_TEXT = 0\n",
    "DO_DESKEW = False\n",
    "REMOVE_BORDER_FRAC = 0.02\n",
    "ADD_BORDER_PADDING = 10\n",
    "ADD_BORDER_COLOR = (255, 255, 255)\n",
    "\n",
    "OCR_LANG = \"Latin+osd\"\n",
    "OCR_CONFIG = \"--psm 6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a6e96-3ec2-42c4-8eab-92f565696af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "def load_img(path):\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Cannot read {path}\")\n",
    "    return img\n",
    "\n",
    "def save_img(path, img):\n",
    "    ext = Path(path).suffix or \".png\"\n",
    "    ok, buf = cv2.imencode(ext, img)\n",
    "    if not ok:\n",
    "        raise IOError(\"Could not encode image\")\n",
    "    buf.tofile(str(path))\n",
    "\n",
    "def order_points(pts):\n",
    "    rect = np.zeros((4,2), dtype=\"float32\")\n",
    "    s = pts.sum(axis=1)\n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    diff = np.diff(pts, axis=1)\n",
    "    rect[1] = pts[np.argmin(diff)]\n",
    "    rect[3] = pts[np.argmax(diff)]\n",
    "    return rect\n",
    "\n",
    "def four_point_transform(image, pts, dst_size=None):\n",
    "    rect = order_points(pts)\n",
    "    (tl, tr, br, bl) = rect\n",
    "    widthA = np.linalg.norm(br-bl)\n",
    "    widthB = np.linalg.norm(tr-tl)\n",
    "    maxWidth = int(max(widthA, widthB))\n",
    "    heightA = np.linalg.norm(tr-br)\n",
    "    heightB = np.linalg.norm(tl-bl)\n",
    "    maxHeight = int(max(heightA, heightB))\n",
    "    if dst_size is not None:\n",
    "        (maxWidth, maxHeight) = dst_size\n",
    "    dst = np.array([[0,0],[maxWidth-1,0],[maxWidth-1,maxHeight-1],[0,maxHeight-1]], dtype=\"float32\")\n",
    "    M = cv2.getPerspectiveTransform(rect, dst)\n",
    "    return cv2.warpPerspective(image, M, (maxWidth, maxHeight), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=(255,255,255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976fa02-49a8-45da-88ab-c837f9600049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Page Detection & Cropping ---\n",
    "def detect_page_and_crop(img_bgr):\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, GAUSSIAN_BLUR_KERNEL, 0)\n",
    "    edges = cv2.Canny(blur, CANNY_LOW, CANNY_HIGH)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, DILATE_KERNEL_SIZE)\n",
    "    edges = cv2.dilate(edges, kernel, iterations=DILATE_ITERATIONS)\n",
    "    contours, _ = cv2.findContours(edges.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "    for cnt in contours:\n",
    "        peri = cv2.arcLength(cnt, True)\n",
    "        approx = cv2.approxPolyDP(cnt, 0.02*peri, True)\n",
    "        if len(approx) == 4 and cv2.contourArea(approx) > MIN_AREA_FRAC*h*w:\n",
    "            return four_point_transform(img_bgr, approx.reshape(4,2).astype(\"float32\"))\n",
    "    # Fallback: central crop\n",
    "    m = int(REMOVE_BORDER_FRAC * min(w,h))\n",
    "    return img_bgr[m:h-m, m:w-m].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6f9e62-ac47-4459-b303-3abe6d78e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Deskew CPU ---\n",
    "def deskew_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape)==3 else image\n",
    "    gray = cv2.bitwise_not(gray)\n",
    "\n",
    "    # ðŸ”§ Ensure dtype is uint8 (OpenCV requirement for OTSU)\n",
    "    if gray.dtype != np.uint8:\n",
    "        gray = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "    coords = np.column_stack(np.where(thresh > 0))\n",
    "    angle = cv2.minAreaRect(coords)[-1]\n",
    "\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "\n",
    "    if abs(angle) < 0.5:\n",
    "        return image\n",
    "\n",
    "    (h, w) = image.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "    return cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824f6d17-c25a-4267-b49e-5f829e9e68e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CPU Preprocessing for OCR ---\n",
    "def preprocess_for_ocr(img_bgr):\n",
    "    if DO_INVERT:\n",
    "        img_bgr = cv2.bitwise_not(img_bgr)\n",
    "    if DO_DESKEW:\n",
    "        img_bgr = deskew_image(img_bgr)\n",
    "\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_rgb = cv2.bilateralFilter(img_rgb, BILATERAL_D, BILATERAL_SIGMA_COLOR, BILATERAL_SIGMA_SPACE)\n",
    "    \n",
    "    lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)\n",
    "    l,a,b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=CLAHE_CLIP_LIMIT, tileGridSize=CLAHE_TILE_GRID)\n",
    "    l2 = clahe.apply(l)\n",
    "    img_rgb = cv2.cvtColor(cv2.merge((l2,a,b)), cv2.COLOR_LAB2RGB)\n",
    "\n",
    "    if DO_BINARIZE:\n",
    "        gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "        # Make sure black text is foreground (0) and background is 255\n",
    "        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, 10)\n",
    "        # thresh is now single-channel: 0 for black text, 255 for white background\n",
    "        img_gray = thresh  # Keep single-channel for morphological ops\n",
    "    else:\n",
    "        img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    if ERODE_ITERATIONS>0:\n",
    "        kernel = np.ones(ERODE_KERNEL_SIZE, np.uint8)\n",
    "        img_gray = cv2.dilate(img_gray, kernel, iterations=DILATE_ITERATIONS_TEXT)\n",
    "        # img_rgb = cv2.erode(img_rgb, kernel, iterations=ERODE_ITERATIONS)\n",
    "        \n",
    "    if DILATE_ITERATIONS_TEXT>0:\n",
    "        kernel = np.ones(DILATE_KERNEL_SIZE_TEXT, np.uint8)\n",
    "        img_gray = cv2.dilate(img_gray, kernel, iterations=DILATE_ITERATIONS_TEXT)\n",
    "        # img_rgb = cv2.dilate(img_rgb, kernel, iterations=DILATE_ITERATIONS_TEXT)\n",
    "\n",
    "    if TARGET_SHORT_SIDE>0:\n",
    "        h,w = img_rgb.shape[:2]\n",
    "        scale = TARGET_SHORT_SIDE / min(h,w)\n",
    "        new_h,new_w = int(h*scale), int(w*scale)\n",
    "        img_gray = cv2.resize(img_gray, (new_w,new_h), interpolation=cv2.INTER_LINEAR)\n",
    "        # img_rgb = cv2.resize(img_rgb, (new_w,new_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    if ADD_BORDER_PADDING>0:\n",
    "        img_gray = cv2.copyMakeBorder(img_gray, ADD_BORDER_PADDING, ADD_BORDER_PADDING, ADD_BORDER_PADDING, ADD_BORDER_PADDING, cv2.BORDER_CONSTANT, value=ADD_BORDER_COLOR)\n",
    "        # img_rgb = cv2.copyMakeBorder(img_rgb, ADD_BORDER_PADDING, ADD_BORDER_PADDING, ADD_BORDER_PADDING, ADD_BORDER_PADDING, cv2.BORDER_CONSTANT, value=ADD_BORDER_COLOR)\n",
    "    return Image.fromarray(img_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947dbb39-a60f-4d13-9507-f49377660902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GPU Preprocessing for OCR ---\n",
    "def preprocess_for_ocr_gpu(img_bgr, device):\n",
    "    # Convert to grayscale and normalize\n",
    "    tensor = torch.from_numpy(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)).unsqueeze(0).unsqueeze(0).float() / 255.0\n",
    "    tensor = tensor.to(device)\n",
    "\n",
    "    # Deskew if enabled\n",
    "    if DO_DESKEW:\n",
    "        tensor_cpu = tensor.squeeze().cpu().numpy()\n",
    "        tensor_cpu = deskew_image(tensor_cpu)\n",
    "        tensor = torch.from_numpy(tensor_cpu).unsqueeze(0).unsqueeze(0).float().to(device) / 255.0\n",
    "\n",
    "    # Resize\n",
    "    if TARGET_SHORT_SIDE > 0:\n",
    "        _, _, h, w = tensor.shape\n",
    "        scale = TARGET_SHORT_SIDE / min(h, w)\n",
    "        new_h, new_w = int(h * scale), int(w * scale)\n",
    "        tensor = F.interpolate(tensor, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "\n",
    "    # Binarization (adaptive thresholding)\n",
    "    if DO_BINARIZE:  \n",
    "        tensor_cpu = tensor.squeeze().cpu().numpy() * 255\n",
    "        tensor_cpu = cv2.adaptiveThreshold(tensor_cpu.astype(np.uint8), 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 15, 10)\n",
    "        tensor = torch.from_numpy(tensor_cpu.astype(np.float32) / 255.0).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    # Morphology (dilate to make text more prominent)\n",
    "    if DILATE_ITERATIONS_TEXT > 0:\n",
    "        kernel = torch.ones(1, 1, *DILATE_KERNEL_SIZE_TEXT, device=device)\n",
    "        for _ in range(DILATE_ITERATIONS_TEXT):\n",
    "            tensor = F.conv2d(tensor, kernel, padding=0)\n",
    "            tensor = torch.clamp(tensor, 0, 1)\n",
    "\n",
    "    # Add border if needed\n",
    "    if ADD_BORDER_PADDING > 0:\n",
    "        tensor = F.pad(tensor, (ADD_BORDER_PADDING,) * 4, mode='constant', value=1.0)\n",
    "\n",
    "    # Convert back to PIL Image\n",
    "    tensor = (tensor.squeeze(0).squeeze(0) * 255).byte().cpu().numpy()\n",
    "    return Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e10c9-edb9-43e2-9605-7ffb6cb36ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OCR ---\n",
    "def ocr_infer_pil(image_pil):\n",
    "    return pytesseract.image_to_string(image_pil, lang=OCR_LANG, config=OCR_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ceb235-999b-4198-b859-b44beb934028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processing_profile():\n",
    "    profile = {\"device\": \"cpu\", \"batch_size\": 1, \"resize\": 1200}\n",
    "    total_ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        total_vram_gb = props.total_memory / (1024**3)\n",
    "\n",
    "        if total_vram_gb >= 8:\n",
    "            profile.update({\"device\": \"cuda\", \"batch_size\": 8, \"resize\": 1800})\n",
    "        elif total_vram_gb >= 4:\n",
    "            profile.update({\"device\": \"cuda\", \"batch_size\": 4, \"resize\": 1500})\n",
    "        else:\n",
    "            profile.update({\"device\": \"cuda\", \"batch_size\": 2, \"resize\": 1200})\n",
    "    else:\n",
    "        if total_ram_gb >= 16:\n",
    "            profile.update({\"batch_size\": 4, \"resize\": 1500})\n",
    "        elif total_ram_gb >= 8:\n",
    "            profile.update({\"batch_size\": 2, \"resize\": 1200})\n",
    "        else:\n",
    "            profile.update({\"batch_size\": 1, \"resize\": 1000})\n",
    "\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b2669-40df-4cbb-b8be-b1c966520c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "def detect_device(prefer_gpu=True):\n",
    "    if prefer_gpu and torch.cuda.is_available():\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return torch.device(\"cuda\")\n",
    "    print(\"Using CPU\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def preview_random_page(files, device):\n",
    "    \"\"\"Pick a random file, preprocess, and display comparison inline.\"\"\"\n",
    "    preview_file = random.choice(files)\n",
    "    print(f\"[Preview] Showing random file: {preview_file.name}\")\n",
    "    \n",
    "    img = load_img(preview_file)\n",
    "    cropped = detect_page_and_crop(img)\n",
    "    preprocessed = preprocess_for_ocr_gpu(cropped, device) if device.type == 'cuda' else preprocess_for_ocr(cropped)\n",
    "\n",
    "    # --- Show original vs preprocessed ---\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "    axes[0].imshow(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title(\"Original Cropped Page\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(preprocessed, cmap='gray')\n",
    "    axes[1].set_title(\"Preprocessed for OCR\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # OCR preview text\n",
    "    text = ocr_infer_pil(preprocessed)\n",
    "    print(f\"--- Preview OCR ---\\n{text[:1000]}\")\n",
    "\n",
    "    # Ask user if they want to proceed\n",
    "    proceed = input(\"Proceed with OCR using these settings? [y/n]: \").strip().lower()\n",
    "    return proceed == \"y\"\n",
    "\n",
    "def ocr_worker(path, device):\n",
    "    try:\n",
    "        img = load_img(path)\n",
    "        cropped = detect_page_and_crop(img)\n",
    "        preprocessed = preprocess_for_ocr_gpu(cropped, device) if device.type == 'cuda' else preprocess_for_ocr(cropped)\n",
    "        return ocr_infer_pil(preprocessed)\n",
    "    except Exception as e:\n",
    "        return f\"Error processing {path}: {e}\"\n",
    "\n",
    "def batch_process_book(book_dir, out_dir, max_workers=4, preview=True):\n",
    "    device = detect_device()\n",
    "    profile = get_processing_profile()\n",
    "    batch_size = profile[\"batch_size\"]\n",
    "    devi_ce = torch.device(profile[\"device\"])\n",
    "    print(f\"Running on {devi_ce}, batch size {profile['batch_size']}, resize {profile['resize']}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            book_dir = Path(book_dir)\n",
    "            out_dir = Path(out_dir)\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            files = sorted([p for p in book_dir.iterdir() if p.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tif', '.tiff']])\n",
    "            \n",
    "            # âœ… Show preview before starting\n",
    "            if preview and files:\n",
    "                approved = preview_random_page(files, device)\n",
    "                if not approved:\n",
    "                    print(\"[INFO] Aborted by user. Please tweak preprocessing parameters and re-run.\")\n",
    "                    return  # exit before OCR\n",
    "\n",
    "            results = []\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                for text in tqdm(executor.map(lambda p: ocr_worker(p, device), files), total=len(files)):\n",
    "                    results.append(text)\n",
    "\n",
    "            out_path = out_dir / f\"{book_dir.name}_ocr.txt\"\n",
    "            with open(out_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"\\n\".join(results))   \n",
    "            print(\"OCR complete:\", out_path)\n",
    "            break  # success\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e) and batch_size > 1:\n",
    "                batch_size = max(1, batch_size // 2)\n",
    "                profile[\"batch_size\"] = batch_size\n",
    "                torch.cuda.empty_cache()\n",
    "                print(f\"[WARN] CUDA OOM - reducing batch size to {batch_size} and retrying...\")\n",
    "                continue\n",
    "            else:\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5a146-0b56-4ad1-945c-81ed6cb06251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [11] --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # -----ðŸ”§ Change these paths to match your book images folder and output folder ------\n",
    "    # Enter the name of the folder in which the pages are in\n",
    "    BOOK_NAME = \"Bhala_Mabhalana\" \n",
    "    # Make sure that you put the files you want to process inside 'data/'raw data'\n",
    "    book_dir = Path.cwd() / \"data\" / \"raw data\" / BOOK_NAME\n",
    "    # Once the OCR is complete navigate to 'data/output' to find the output file\n",
    "    out_dir = Path.cwd() / \"data\" / \"output\" \n",
    "\n",
    "    print(f\"Starting OCR pipeline on: {book_dir}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    profile = get_processing_profile()\n",
    "    batch_process_book(book_dir, out_dir, max_workers=profile[\"batch_size\"], preview=True)\n",
    "\n",
    "    print(f\"OCR Pipeline complete in {time.time() - start_time:.2f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1fce1-51f8-42c5-b8f1-7011e59e5a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (crazyocr)",
   "language": "python",
   "name": "crazyocr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
